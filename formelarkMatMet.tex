\documentclass[a4paper,norsk, 10pt]{article}
\usepackage[utf8]{inputenc}
\usepackage{verbatim}
\usepackage{listings}
\usepackage{graphicx}
\usepackage[norsk]{babel}
\usepackage{a4wide}
\usepackage{color}
\usepackage{amsmath}
\usepackage{float}
\usepackage{amssymb}
\usepackage[dvips]{epsfig}
\usepackage[toc,page]{appendix}
\usepackage[T1]{fontenc}
\usepackage{cite} % [2,3,4] --> [2--4]
\usepackage{shadow}
\usepackage{hyperref}
\usepackage{titling}
\usepackage{marvosym }
\usepackage{subcaption}
\usepackage[noabbrev]{cleveref}
\usepackage{cite}


\setlength{\droptitle}{-10em}   % This is your set screw

\setcounter{tocdepth}{2}

\lstset{language=c++}
\lstset{alsolanguage=[90]Fortran}
\lstset{alsolanguage=Python}
\lstset{basicstyle=\small}
\lstset{backgroundcolor=\color{white}}
\lstset{frame=single}
\lstset{stringstyle=\ttfamily}
\lstset{keywordstyle=\color{red}\bfseries}
\lstset{commentstyle=\itshape\color{blue}}
\lstset{showspaces=false}
\lstset{showstringspaces=false}
\lstset{showtabs=false}
\lstset{breaklines}
\title{FYS3140 Oblig1}
\author{Daniel Heinesen, daniehei}
\begin{document}

\section{Kompleks analyse:}
\subsection{Komplekse tall:}
\begin{equation}
z = x+iy, \qquad \bar{z} = x - iy,\qquad z=r(\cos \theta + i\sin\theta)= re^{i\theta}
\end{equation}
$r = |z|$ er modulus, $\theta$ er argumentet.
\begin{equation}
z + \bar{z} = 2Re(z)\qquad z - \bar{z} = 2Im(z), \qquad z\bar{z} = |z|^2 = r^2 = x^2 + y^2
\end{equation}
\begin{equation}
x = \frac{1}{2}(z + \bar{z}),\qquad y = \frac{1}{2i}(z-\bar{z})
\end{equation}
$z-z_0| < R$ er alle $z$ innenfor en radius $R$
\subsection{Komplekse rÃ¸tter:}
\begin{equation}
z^{1/n} = \sqrt[n]{r}e^{i\theta/n} = \sqrt[n]{r}(\cos\theta/n + i\sin\theta/n) = \omega_0
\end{equation}
Dette gir bare 'the principal root', resten er gitt ved
\begin{equation}
\omega_k = \sqrt[n]{r}e^{i\frac{\theta + 2\pi k}{n}}
\end{equation}

\subsection{Analytic functions:}
Def: A function is analytic in a region of the complex plane if it has a (unique) derivative at every point in that region.\\

All analytic functions can be written in terms of $z = x+iy$ alone.
\subsubsection{Cauchy-Riemann equation:}
\begin{equation}
f(z) = u(x,y) + iv(x,y)
\end{equation}
\begin{equation}
\frac{\partial u}{\partial x} = -\frac{\partial v}{\partial y}, \qquad \frac{\partial v}{\partial x} = \frac{\partial v}{\partial y}
\end{equation}
If this holds in a region, that $f$ is analytic in this region, and vice versa. 

\begin{itemize}
\item Regular points: $f(z)$ is analytic
\item Singular point/singularities: A point where $f(z)$ is not analytic.
\item Isolated singularity: a point where $f$ is not analytic, but is a limit of points where $f$ is analytic.
\end{itemize}

If $f$ is analytic in some region, it has first order derivatives, then it also has derivatives of all orders in that region.

\subsubsection{Harmonic Functions:}
If $f = u + iv$ is analytic in a region, then $u$ and $v$ are harmonic:
\begin{equation}
\frac{\partial^2 u}{\partial x^2} + \frac{\partial^2 u}{\partial y^2} = 0\qquad \frac{\partial^2 v}{\partial x^2} + \frac{\partial^2 v}{\partial y^2} = 0
\end{equation}
If $u$ is harmonic, one can find a $v$ such that $f = u + iv$ is analytic. $v$ is the harmonic conjugate of $u$.

\subsection{Contour integrals:}
\begin{equation}
\int_{\Gamma} f(z) dz = \lim_{z\rightarrow \infty} \sum_{k=1}^{\infty} f(c_k) \Delta z_k
\end{equation}
For a generalized curve with parametrization $z(t)$:
\begin{equation}
\int_{\gamma}f(t)dz = \int_a^b f(z(t))z'(t)dt
\end{equation}
\subsubsection{An important integral:}
$C-r = |z-z_0| = r$:
\begin{equation}
I = \int_{C_r}(z-z_0)^n dz = 
\begin{cases}
0 & n \neq -1\\
2\pi i & n = -1
\end{cases}
\end{equation}
\subsubsection{Upper bound estimate:}
Generalized triangle inequality:
\begin{equation}
\bigg| \sum_k z_k\bigg| \leq \sum_k|z_k|
\end{equation}
Applied to Riemann sum we get the upper bound estimate:
\begin{equation}
\bigg| \int_{\gamma}f(z) dz\bigg| \leq ML
\end{equation}
Where $M = \max|f(z)|$ and $L$ is the length of the curve.

\subsubsection{Path:}
If $f$ is continuous everwhere in $D$, then contour integrals are independent of paths, and any loop integral is zero. One can also deform a contour without crossing any singularities and get that:
\begin{equation}
\int_{\Gamma_1}f(z) dz = \int_{\Gamma_2}f(z) dz
\end{equation}

\subsubsection{Cauchy Theorem:}
If $f$ is analytic in a simply connected domain $D$ with no singularities, and $\Gamma$ is any closed contour in $D$, then
\begin{equation}
\int_{\Gamma}f(z) dz = 0
\end{equation}
\subsubsection{Cauchy's integral formula:}
Let $\Gamma$ be a simple, closed, positively oriented contour. Assume $f$ is analytic in some simply connected domain $D$ containing $\Gamma$, and some $z_0$ is inside $\Gamma$. Then:
\begin{equation}
f(z_0) = \frac{1}{2\pi i}\int_{\Gamma}\frac{f(z)}{z-z_0}dz
\end{equation}
\subsubsection{Generalized Cauchy integral formula:}
\begin{equation}
f^{(n)}(z) = \frac{n!}{2\pi i}\oint_{\Gamma}\frac{f(w)}{(z-w)^{n+1}}dw
\end{equation}
\subsubsection{Cauchy inequality:}
Let $f$ be analytic on and inside a circle($C_r$) of radius R, centered at $z_0$. If $|f(z)|\leq M$ for some $z$ on $C_r$, then the derivatives satisfy:
\begin{equation}
|f^{(n)}(z_0)| \leq \frac{n!M}{R^n}
\end{equation}
This gives Liuvilles theorem: A function which is analytic and bounded in the entire complex plane, is constant.
\subsection{Taylor and Laurent Series:}
\subsubsection{Taylor:}
\begin{equation}
f(z) = f(z_0) + f'(z_0)(z-z_0) + \frac{1}{2!}f''(z_0)(z-z_0)^2 + ... = \sum_n \frac{f^{(n)}(z_0)}{n!}(z-z_0)^n
\end{equation}
If $f$ is analytic in the disk $|z - z_0|<R$ then the above Taylor series converges in that disk.[i.e. the disk touching the nearest singularity]\\

If $f$ is analytic at $z_0$, then the Taylor series for $df/dz$ can be obtained by termwise differentiation.

\subsubsection{Laurent:}
Let $f$ be analytic in the annulus $r < |z-z_0| < R$. Then $f$ can be expanded there as the sum of two series:
\begin{equation}
f(z) = \sum_{k=0}^{\infty} a_k(z-z_0)^k +\sum_{k=1}^{\infty} b_k(z-z_0)^{-k} 
\end{equation}
With
\begin{equation}
a_n = \frac{1}{2\pi i}\oint_{\Gamma}\frac{f(z)}{(z-z_0)^{n+1}}dz\qquad b_n = \frac{1}{2\pi i}\oint_{\Gamma}\frac{f(z)}{(z-z_0)^{-n+1}}dz
\end{equation}
Def: The coefficient $b_1$ of the $1/(z-z_0)$ term is the residue of $f(z)$ at $z = z_0$.

Laurent series are unique. So to find them we can use

\begin{equation}
\frac{1}{1-\omega} = \sum_{n=0}^{\infty}\omega^2 \text{ , when } |\omega|<1
\end{equation}

\subsection{Zeros:}
\begin{itemize}
\item A zeros of a function is a point where $f$ is analytic and $f(z_0) = 0$
\item A zeros of order $m$: $f(z_0) = f'(z_0) = ...= f^{m-1}(z_0) = 0$, $f^m(z_0) \neq 0$
\item Can be factorized as: $f(z) = (z-z_0)^m \cdot g(z)$, where $g(z)$ is analytic and $g(z_0) \neq 0$
\end{itemize}
\subsection{Isolated singularities:}
Let $f$ have a Laurent series, then we can have:
\subsubsection{Removable Singularity/Regular point:}
If all $b_n = 0$ at $z_0$. $f(z)$ has a limit $z\rightarrow z_0$ and we can be redefined such that $f$ is analytic at $z_0$.
\subsubsection{Essential Singularity:}
Infinitely many b-terms at $z_0$
\subsubsection{Pole of order $m$}
Order $m$ is the highest exponent of the $1/(z-z_0)$ terms.
\begin{equation}
f(z) = \frac{b_m}{(z-z_0)^m}+ ... + \frac{b_1}{z-z_0} + a_0 + a_1(z-z_0) +...
\end{equation}

 $f(z)$ can be written as $\frac{g(z)}{(z-z_0)^m}$. A pole of order 1 $(m = 1)$ is a Simple pole.
 
\subsection{Residue Theory:}
\subsubsection{Residue Theorem:}
If $\Gamma$ is a simple, closed, positively oriented contour, and $f$ is analytic on and inside $\Gamma$ except at the points $z_0,z_1,...,z_n$ inside $\Gamma$, then
\begin{equation}
\oint_{\Gamma}f(z)dz = 2\pi i \sum_{k=0}^n Res(f,z_k)
\end{equation}

\subsubsection{Determining the residues:}
1: Read off $b_1$ from the Laurent series.\\

2: Simple poles:
\begin{equation}
Res(z_0) = b_1 = \lim_{z\rightarrow z_0}(z-z_0)f(z)
\end{equation}
Finite answer only if the pole is of first order. 
\begin{equation}
f(z) = \frac{P(z)}{Q(z)} \Rightarrow Res(z_0) = \frac{P(z_0)}{Q'(z_0)}
\end{equation}


3: Multiple poles:
If $f$ has a pole of order $m$ at $z_0$, then
\begin{equation}
Res(z_0) = \lim_{z \Rightarrow z_0}\left[\frac{1}{(m-1)!}\frac{d^{m-1}}{dz^{m-1}}\left((z-z_0)^mf(z))\right)\right]
\end{equation}

Ok to overshoot with value if $m$.

\subsection{Applications to Real Integrals:}
\subsubsection{Type 1:}
Rational and finite functions of $\sin\theta$ and $\cos\theta$ over the interval$[0,2\pi]$. Use:
\begin{itemize}
\item $z = e^{i\theta}$, \qquad$d\theta = dz/iz$
\item $\cos\theta = \frac{1}{2}(e^{i\theta} + e^{-i\theta}) = \frac{1}{2}(z+1/z)$,\qquad $\sin\theta = \frac{1}{2i}(e^{i\theta} - e^{-i\theta}) = \frac{1}{2i}(z-1/z)$
\item then use residue theorem.
\end{itemize}
\subsubsection{Type 2a}
Integrals of rational functions from $-\infty$ to $\infty$
\begin{equation}
I = \int_{-\infty}^{\infty} \frac{P(x)}{Q(x)}dx
\end{equation}
\begin{itemize}
\item Make a contour $\gamma_{\rho}$ from $-\rho$ to $\rho$
\item Add a second contour from $\rho$ via a the complex plane(half circle in the upper part of the complex plane) back to $-\rho$, $C_{\rho}$.
\item use the residue theorem. Remember that the singularities have to be in the upper part
\item Show that the contribution form $C_{\rho}$ vanishes as $\rho \rightarrow \infty$
\end{itemize}

\subsubsection{Type 2b:}
\begin{equation}
I = \int_{-\infty}^{\infty} \frac{P(x)}{Q(x)}\cos(mx)dx, \qquad I = \int_{-\infty}^{\infty} \frac{P(x)}{Q(x)}\sin(mx)dx
\end{equation}

\textbf{Alt 1(Always safe):}

use $\cos(mx) = \frac{1}{2}(e^{imx} + e^{-imx})$
\begin{equation}
I = \frac{1}{2}\int_{-\infty}^{\infty} \frac{P(x)}{Q(x)}e^{imx}dx + \int_{-\infty}^{\infty} \frac{P(x)}{Q(x)}e^{-imx}dx
\end{equation}
FOr the first term, use a closed contour in the upper part of the complex plane, for the second term use one in the lower half.

\textbf{Alt 2: safe as long as P/Q is real.}
Note that $\cos(mx) = Re (e^{mx})$(and $\sin(mx) = Im (e^{mx})$). We can therfor use $\cos(mx) \rightarrow e^{imx}$ and then take the real part at the end(or the imaginary if we have $\sin(mx)$

\subsubsection{Jordan's lemma:}
If $m>0$(real) and P/Q is the quotient of two polynomial such that $deg(Q) \geq deg(P) + 1$, then
\begin{equation}
\lim_{\rho\rightarrow \infty}\int_{C_{\rho}^+}\frac{P}{Q}e^{imz}dz = 0
\end{equation}
Where $C_{\rho}^+$ is the contour in the upper plane. Same holds for $m<0$ but with $C_{\rho}^-$ in the lower plane.

\subsubsection{Type 3:}
Singularities on the real plane. We get Principal Values.
\begin{equation}
PV\int_a^b f(x) dx = \lim_{r\rightarrow 0}\int_a^{c-r}f(x) dx + \int_{c+r}^b f(x) dx
\end{equation}
Where $c$ is a singularity. If the singularities are simple poles the integral can be calculated with the residue theorem.
\begin{equation}
PV\int_{-\infty}^{\infty}f(x)dx = 2\pi i\sum_k Res(z_k) + \pi i \sum_j Res(z_j)
\end{equation}
Where $z_k$ are singularities on the upper half plane, and $z_j$ are singularities on the real line.

\section{Tensor:}
\subsection{Cartesian Tensor}
Transform properly under rotation of Cartesian coordinate system.
\begin{equation}
e'_i\cdot e_j = \cos\theta_{i'j} \equiv A_{ij}
\end{equation}
\subsubsection{Transformation of a position vector}
\begin{equation}
\vec{r} = x_i e_i = x'_j e'_j
\end{equation}
\begin{equation}
x'_i = \vec{r}\cdot e'_i = e'_i\cdot e_jx_j = A_{ij}x_j
\end{equation}
Reverse:
\begin{equation}
x_i = e_i\cdot\vec{r} = e_i\cdot e'_jx'_j = A_{ji}'x_j = A_{ij}^T x'_j
\end{equation}
\begin{equation}
A^{-1} = A^T
\end{equation}
\subsubsection{Cartesian vector/tensor}
\begin{equation}
v' = Av, \qquad T'_{kl} = A_{ki}A_{lj}T_{ij}
\end{equation}
\begin{equation}
\vec{j}=\sigma \vec{E} \text{ Ohm's law}
\end{equation}
\subsubsection{Inertia Tensor}
Rigid body rotation around fixed axis:
\begin{equation}
\vec{L} = I\vec{\omega} 
\end{equation}
Rotation around a point. $I$ is a rank 2 tensor
\begin{equation}
L_i = I_{ij}\omega_j
\end{equation}
Determined from
\begin{equation}
\vec{L} = \sum_k m_kr_k\times(\omega\times r_k)
\end{equation}
Uniform mass density: sum goes to integral.$I$ is symmetric: can find coord. system in which $I$ is diagonal. Eigenvetors (axes of new coord. system): Principal axes of inertia.
\subsection{Levi-Civita and Kronicker Delta}
\begin{equation}
\delta_{ij} = 
\begin{cases}
1 & i = j\\
0
\end{cases}
\end{equation}
\begin{equation}
\epsilon_{ijk} = 
\begin{cases}
1 & \text{ even permutation}\\
-1 & \text{ odd permutation}\\
0
\end{cases}
\end{equation}
\begin{equation}
\epsilon_{ijk}\epsilon_{imn} = \delta_{jm}\delta_{kn} - \delta_{jn}\delta_{km}
\end{equation}
Inner product: $\vec{u}\cdot\vec{c} = u_iv_j\delta_{ij} = u_iu_i$\\
Cross Product: $\vec{A}\times\vec{B} = \epsilon_{ijk}A_jB_k$\\
Curl: $(\nabla\times\vec{V})_i = \epsilon_{ijk}\frac{\partial}{\partial x_j}V_k$
\section{Calculus of Variation:}
Minimize:
\begin{equation}
I = \int_{x_1}^{x_2}F(x,y,y') dx
\end{equation}
\begin{equation}
\frac{\partial F}{\partial y} - \frac{d}{dx}\left(\frac{\partial F}{\partial y'}\right) = 0
\end{equation}
Can simplify Euler-Lagrange by change of variables:
\begin{equation}
x' = \frac{1}{y'}, \qquad dx = x'dy \text{ or } dy = y'dx
\end{equation}
\subsection{Optics -- Fermat's Principle:}
\begin{equation}
P = \int n ds, \qquad ds = \sqrt{dx^2 + dy^2}
\end{equation}

\section{ODE:}
\subsection{Linear First order DE:}
\begin{equation}
y' + P(x)y = Q(x)\qquad dy + [Py - Q]dx = 0
\end{equation}
\subsubsection{Exact DE's}
\begin{equation}
du = \frac{\partial u}{\partial x} dx + \frac{\partial u}{\partial y}dy, \qquad M(x,y)dx + N(x,y)dy = 0
\end{equation}
The last DE is called exact if the LHS is a total differential,
\begin{equation}
M = \frac{\partial u}{\partial x}; \qquad N = \frac{\partial u}{\partial y}
\end{equation}
This gives:
\begin{equation}
du = 0 \Leftrightarrow u = \text{ const}
\end{equation}
\subsubsection{Integrating factor:}
\begin{equation}
y(x) = \frac{1}{\mu(x)}\left[\int \mu(x)Q(x)dx + C\right], \qquad \mu(x) = e^{\int P(x)dx}
\end{equation}
If $Q(x) = 0$, the homogeneous case:
\begin{equation}
y(x) = \frac{C}{\mu(x)}=Ce^{-\int P(x)dx}
\end{equation}

\subsection{Ordinary 2nd order DE:}
\begin{equation}
y'' + P(x)y' + Q(x)y = R(x), \qquad y(x) = y_h + y_p = c_1y_1 + c_2y_2 + y_p
\end{equation}
If $y_1$ and $y_2$ are linearly independent, it must hold that the Wronskian determinant
\begin{equation}
W(x_0) = 
\begin{vmatrix}
y_1(x_0) & y_2(x_0)\\
y_1'(x_0) & y_2'(x_0)
\end{vmatrix}
\neq 0 \qquad \forall x_0
\end{equation}
\subsection{Homogeneous equations ($R(x) = 0)$}
\subsubsection{Variations of Constants:}
if $y_1$ is a solution, an other linearly independent solution can be found as
\begin{equation}
y_2(x) = c(x)y_1(x)
\end{equation}
Where $c(x)$ can be found from the DE.
\subsubsection{Homogeneous DE's with constant coefficients}
\begin{equation}
y'' + ay' + by = 0, \qquad \lambda^2 + a\lambda + b = 0 \qquad \lambda_{\pm} = \frac{1}{2}[-a\pm\sqrt{a^2-4b}]
\end{equation}
\textbf{Case 1:}
$\lambda_- \neq \lambda_+$, both real $(a^2-4b > 0)$, gives two linearly independent solutions:
\begin{equation}
y(x) = C_1e^{\lambda_+ x} + C_2e^{\lambda_- x}
\end{equation}
\textbf{Case 2:}
Double root $\lambda_+ = \lambda_- \equiv \lambda = \frac{-a}{2}$
\begin{equation}
y(x) = (Ax + B)e^{\lambda x}
\end{equation}
\textbf{Case 3:}
Complex root $(a^2-4b < 0)$
\begin{equation}
\lambda_{\pm} = -\frac{a}{2}\pm i \sqrt{4b - a^2} = -\frac{a}{2}\pm i\omega
\end{equation}
\begin{equation}
y(x) = Ae^{-a/2 + i\omega} + Be^{-a/2 - i\omega} = e^{-ax/2}\left(A\cos(\omega x) + B\sin(\omega x)\right)
\end{equation}
\begin{equation}
=e^{-ax/2}\left(\tilde{A}e^{i\omega x} + \tilde{B}e^{-i\omega x}\right) = ke^{-ax/2}\sin(\omega x + \phi)
\end{equation}
\subsubsection{Euler-Cauchy:}
\begin{equation}
x^2y'' + a_1xy' + a_0y=0, \text{  or  } y'' +\frac{a_1}{x}y' + \frac{a_0}{x^2}y = 0
\end{equation}
Use:
\begin{equation}
x = e^z, \qquad z = \ln x, \qquad dx = e^zdz = xdz
\end{equation}
For $x > 0$
\begin{equation}
x = -|x| =  -e^z, \qquad z = \ln |x|, \qquad dx = -e^zdz = xdz
\end{equation}
For $x < 0$. We then get
\begin{equation}
\frac{d^2y(z)}{dz^2} + (a_1 - 1)\frac{dy(z)}{dz}+ a_0y(z) = 0
\end{equation}
Solve then transform back to $y(x)$. No solution at $x = 0$, and different coefficients for cases $x<0$ and $x>0$!

\subsection{Inhomogeneous DE}
\begin{equation}
y(x) = y_h(x) + y_p(x)
\end{equation}
\subsubsection{Methods of Undetermined Coefficients:}
\begin{equation}
y'' + ay' + by = R(x)
\end{equation}
Where $R(x)$ is simple. If $R(x)$ is a sum of simple functions, then $y_p$ is also a sum.

\textbf{Case 1:}
$R(x) = Ae^{kx}$. Assume $\alpha$, $\beta$ are the roots of the characteristic equation of the homogeneous solution.

\textbf{a)}
If $k \neq \alpha,\beta$. Try $y_p = Be^{kx}$(Find B from the DE)

\textbf{b)}
$k = \alpha$ or $k = \beta$. Try $y_p = Cxe^{kx}$

\textbf{c)}
$k=\alpha=\beta$, so $y_h = (Ax + B)e^{kx}$, try: $y_p = Dx^2e^{kx}$

\textbf{Case 2:}
$R(x) = A\sin kx$ or $ \cos kx$. Has the form $y_p = B\cos kx + C\sin kx$. Efficient: Solve or $\tilde{R}(x) = e^{ikx}$ and take Re or Im at the end.

\textbf{Case 3:}
$R(x) = e^{kx}P_n(x)$. $P_n$ and $Q_n$ are here polynomials of order $n$.

\textbf{a)}
If $k \neq \alpha,\beta$. Try $y_p = Q_n(x)e^{kx}$(Find B from the DE)

\textbf{b)}
$k = \alpha$ or $k = \beta$. Try $y_p = xQ_n(x)e^{kx}$

\textbf{c)}
$k=\alpha=\beta$, so $y_h = (Ax + B)e^{kx}$, try: $y_p = x^2Q_n(x)e^{kx}$

\subsubsection{More General: $y_p$ from factorization:}
\begin{equation}
y'' + P(x)y' + Q(x)y = R(x)
\end{equation}
Assume $u(x)$ is a known solution of the homogeneous DE. Make the ansatz $y_p = u(x)v(x)$. $v(x)$ is found from inserting into the DE. If we define $v' = w$ we'll get
\begin{equation}
w' + \left(\frac{2u}{u'} + P\right)w = \frac{R}{u}
\end{equation}
Solve by integrating factor. And then get 
\begin{equation}
v = \int w dx
\end{equation}

\subsubsection{Variation of Parameters:}
NB! Write DE on the standard form $y'' + Py' + Q = R$. This works if the homogeneous solutions are fully known!
\begin{equation}
y_p(x) = -y_1\int \frac{y_2 R}{W}dx + y_2\int \frac{y_1R}{W} dx
\end{equation}
Where $W$ is the Wronskian
\begin{equation}
W(x_0) = 
\begin{vmatrix}
y_1(x_0) & y_2(x_0)\\
y_1'(x_0) & y_2'(x_0)
\end{vmatrix}
\end{equation}
\subsection{FrÃ¶benius method:}
Any DE of the form
\begin{equation}
y'' + \frac{B(x)}{x}y' + \frac{C(x)}{x^2}y = 0
\end{equation}
Where $B$ and $C$ are analytic in the singular point $x = 0$(if the whole equation is analytic here, we can use power series).
\begin{equation}
y(x) = x^s\sum_n a_nx^n
\end{equation}
\subsubsection{Indicial equation:}
Write $x^2y'' + xb(x)y' + c(x)y = 0$, with $b(x) = b_0 + b_1x^1 + ...$, $c(x) = c_0 + c_1x^1 + ...$ and $y(x) = a_0x^s + a_1x^{s+1} + ...$. If we insert these series in to the DE, and equate the coefficients for the lowest possible power $x^s$ we get
\begin{equation}
s(s-1) + b_0s_0+c_0 = 0
\end{equation}

\textbf{a)}
Two distinct roots $s_1,s_2$ and $s_1 - s_2 \neq $ integer: Two linearly independent solutions.
\begin{equation}
y_i = x^{s_i}\sum_na_nx^n
\end{equation}

\textbf{b)}
Two distinct roots $s_1,s_2$ but $s_1 - s_2 =$ integer. Choose $s_1 > s_2$
\begin{itemize}
\item Often $s_2$ gives a complete solution. Always try the smallest root first.
\item Sometimes $s_2$ does not give a solution, $s_1$ always does: Find $y_1$, then $y_2 = c(x)y_1$ from variations of constants.
\item Double root $s_1 = s_2 = s$. Then find $y_1$ from $s$, then $y_2 = c(x)y_1$
\end{itemize}

\subsubsection{Hermite Eq:}
From harmonic oscillator: $-\psi'' + x^2\psi = (2n+1)\psi$. Use factorization $\psi(x) = e^{-x^2/2}y(x)$. We then get the Hermite DE:
\begin{equation}
y'' - 2xy' +ny = 0
\end{equation}
Solution
\begin{equation}
a_{k+2} = -\frac{2(n-k)}{(k+1)(k+2)}a_k, \qquad H_n(x) = (-1)^ne^{x^2}\frac{d^2}{dx^2}e^{-x^2}
\end{equation}

\subsection{Greens functions:}
\begin{equation}
Dy(x) = R(x), \qquad D = \frac{d^2}{dx^2} + P(x)\frac{d}{dx} + Q(x)
\end{equation}
\begin{equation}
DG(x,z) = \delta(x-z), \qquad y(a) = y(b) = 0
\end{equation}
\begin{equation}
y(x) = \int_a^bG(x,z)R(x)dz 
\end{equation}
$G$ is continuous at $x = z$, but its derivatives has a discontinuity of $1$
\begin{equation}
G(z+\epsilon,z) - G(x-\epsilon,z) = 0, \qquad G'(z+\epsilon,z) - G'(x-\epsilon,z) = 1
\end{equation}

\section{Fourier Series:}
\subsection{Some orthogonality Relations:}
\begin{equation}
\frac{1}{2\pi}\int_{-\pi}^{\pi}\cos(nx)dx = \frac{1}{2\pi}\int_{-\pi}^{\pi}\sin(nx)dx = 0
\end{equation}
\begin{equation}
\frac{1}{2\pi}\int_{-\pi}^{\pi}\cos(mx)\sin(nx)dx = 0
\end{equation}
\begin{equation}
\frac{1}{2\pi}\int_{-\pi}^{\pi}\cos(nx)\cos(mx)dx
=\begin{cases}
0 & n\neq m\\
1/2 & n = m \neq 0\\
1 & n=m=0
\end{cases}
\end{equation}
\begin{equation}
\frac{1}{2\pi}\int_{-\pi}^{\pi}\sin(nx)\sin(mx)dx
=\begin{cases}
0 & n\neq m\\
1/2 & n = m \neq 0\\
0 & n=m=0
\end{cases}
\end{equation}
\begin{equation}
\int_{-\pi}^{\pi}e^{inx}e^{-imx}dx = 2\pi\delta_{nm}
\end{equation}

\subsection{Dirichlet Conditions:}
Sufficient conditions for the Fourier series to converge:
\begin{itemize}
\item A finite number of maxima and minima in the basic interval
\item A finite number of finite discontinuities [bonded]
\item At point $y_0$ where $f(x)$ has a discontinuity, the Fourier series converges at the midpoint.
\item The Fourier Series my be integrated term by term
\item If $f'(x)$ satisfies the Dirichlet conditions, the Fourier series may be differentiated term by term
\end{itemize}


\subsection{Interval $[0,2\pi]$:}
\begin{equation}
f(x) = \frac{1}{2}a_0 + \sum_{n=1}^{\infty}a_n\cos(nx) + \sum_{n=1}^{\infty}b_n\sin(nx) 
\end{equation}
\begin{equation}
a_n = \frac{1}{\pi}\int_{-\pi}^{\pi}f(x)\cos(nx)dx, \qquad b_n = \frac{1}{\pi}\int_{-\pi}^{\pi}f(x)\sin(nx)dx
\end{equation}

\subsection{Complex Series:}
\begin{equation}
f(x) = \sum_{n = -\infty}^{\infty}c_ne^{inx}, \qquad c_k = \frac{1}{2\pi}\int_{-\pi}^{\pi}f(x)e^{-ik x}dx
\end{equation}
Can find a sin-cos series from the complex, and vice versa.

\subsection{Other intervals:}
We can simply change the interval to another basic interval of length $2L$:
\begin{equation}
x\rightarrow \frac{\pi x}{L}, \qquad \frac{1}{\pi}\int_{-\pi}^{\pi} \rightarrow \frac{1}{L}\int_{-L}^{L}
\end{equation}
\subsection{Even and Odd Functions:}
Even: $f(-x) = f(x)$, ex: ($\cos x)$\\
Odd: $f(-x) = -f(x)$, ex: $(\sin x)$\\
$even\cdot even = odd\cdot odd = even$, $even\cdot odd = odd$

\begin{equation}
\int_{-L}^L f(x) dx = 
\begin{cases}
0 & \text{f is odd}\\
2\int_{0}^L f(x) dx & \text{f is even}
\end{cases}
\end{equation}

\subsubsection{Fourier of odd Function, sine-series:}
\begin{equation}
f(x) = \sum_{n = 1}^{\infty}b_n\sin\left(\frac{n\pi x}{L}\right), \qquad b_n = \frac{2}{L}\int_{-L}^L f(x)\sin\left(\frac{n\pi x}{L}\right) dx
\end{equation}

\subsubsection{Fourier of even Function, cos-series:}
\begin{equation}
f(x) = \frac{a_0}{2} + \sum_{n = 1}^{\infty}a_n\cos\left(\frac{n\pi x}{L}\right), \qquad a_n = \frac{2}{L}\int_{-L}^L f(x)\cos\left(\frac{n\pi x}{L}\right) dx
\end{equation}\\

A given function may be represented by several different Fourier Series depending on the physics/context. Given a function defined only on half the interval $[0,L]$. We may either define an \textbf{even} extension of $f(x)$ to the period $2L$ by only using the cosine-series; or we may define an \textbf{odd} extension by using the sine-series.

\subsection{Parseval's Theorem:}
\begin{equation}
\frac{1}{2L}\int_{-L}^L |f(x)|^2dx = \sum_{-\infty}^{\infty}|c_n|^2 = \left(\frac{1}{2}a_0\right)^2 + \frac{1}{2}\sum_{n=1}^{\infty}a_n^2 + \frac{1}{2}\sum_{n=1}^{\infty}b_n^2
\end{equation}
\begin{equation}
\int_{-\infty}^{\infty}|f(x)|^2 = \int_{-\infty}^{\infty}|F(k)|^2 
\end{equation}

\section{Fourier Transforms:}
\begin{equation}
F(k) = \frac{1}{\sqrt{2\pi}}\int_{-\infty}^{\infty} f(x) e^{-ikx} dx = \mathcal{F}[f(x)]
\end{equation}
\begin{equation}
f(x) = \frac{1}{\sqrt{2\pi}}\int_{-\infty}^{\infty} F(k) e^{ikx} dk
\end{equation}
\subsection{Fourier Integral Theorem:}
\begin{equation}
f(x) = \frac{1}{2\pi}\int_{-\infty}^{\infty}e^{ikx}\underbrace{\int_{-\infty}^{\infty}f(\tilde{x}) e^{-ik\tilde{x}}d\tilde{x}}_{\sqrt{2\pi}F(k)}dk
\end{equation}
Is OK if $f(x)$ satisfied the Dirichlet conditions on every finite interval, and $\int_{-\infty}^{\infty}|f(x)|dx$ is finite.

\subsection{Fourier Transforms of Derivatives:}
\begin{equation}
\mathcal{F}[f^{(n)}(x)] = (ik)^n\mathcal{F}[f(x)]
\end{equation}
\subsubsection{With DE:}
\begin{equation}
y'' + ay' + b = f(x), \qquad Y(k) = \mathcal{F}[y(x)] = \frac{F(k)}{-k^2 + aik + b}
\end{equation}
\begin{equation}
y(x) = \mathcal{F}^{-1}[Y(k)] = \frac{1}{\sqrt{2\pi}}\int_{-\infty}^{\infty}e^{ikx}Y(k) dk
\end{equation}
\subsection{Sin and cos-transformations}





\end{document}

